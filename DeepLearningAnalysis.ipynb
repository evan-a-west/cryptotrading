{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd02db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from dask import delayed\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, date\n",
    "from scipy import signal\n",
    "from dask.dataframe.utils import make_meta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:60612' processes=8 threads=16, memory=7.45 GiB>"
      ],
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:60612</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>8</li>\n  <li><b>Cores: </b>16</li>\n  <li><b>Memory: </b>7.45 GiB</li>\n</ul>\n</td>\n</tr>\n</table>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "client = Client(n_workers=8, threads_per_worker=2, memory_limit='1GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE_CRITICAL_POINTS = .8 #percentage of MIN and MAX data devoted to training\n",
    "NEITHER_SIZE_MULTIPLE = 10 #Size of the non-critical point data set in multiples of the critical-point dataset size (i.e. for undersampling the non-critical point dataset)\n",
    "TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_resampled_dask = dd.read_csv(\"DataFromDeepLearningProcessing/DOGE_Deep_2021-04-16.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['datetime', 'mark_price', 'ask_price', 'bid_price', 'spread',\n",
       "       'mark_price_10T_velocity', 'mark_price_60T_velocity',\n",
       "       'mark_price_1440T_velocity', 'mark_price_10T_mean',\n",
       "       'mark_price_60T_mean', 'mark_price_1440T_mean', 'mark_price_10T_std',\n",
       "       'mark_price_60T_std', 'mark_price_1440T_std',\n",
       "       'mark_price_10T_acceleration_for_10T_velocity',\n",
       "       'mark_price_60T_acceleration_for_60T_velocity', 'datetimeNotTheIndex',\n",
       "       'min', 'max', 'minmax'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data_resampled_dask.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_resampled_dask = dd.read_csv(\"DOGE_Deep_2021-04-16.csv\")\n",
    "data_resampled_dask[\"datetime\"] = dd.to_datetime(data_resampled_dask[\"datetime\"])\n",
    "data_resampled_dask[\"datetimeNotTheIndex\"] = dd.to_datetime(\n",
    "    data_resampled_dask[\"datetimeNotTheIndex\"])\n",
    "\n",
    "data_resampled_dask = data_resampled_dask.set_index(\"datetime\", sorted=True)\n",
    "\n",
    "training_columns = ['mark_price', 'ask_price', 'bid_price', 'spread',\n",
    "                    'mark_price_10T_velocity', 'mark_price_60T_velocity',\n",
    "                    'mark_price_1440T_velocity', 'mark_price_10T_mean',\n",
    "                    'mark_price_60T_mean', 'mark_price_1440T_mean', 'mark_price_10T_std',\n",
    "                    'mark_price_60T_std', 'mark_price_1440T_std',\n",
    "                    'mark_price_10T_acceleration_for_10T_velocity',\n",
    "                    'mark_price_60T_acceleration_for_60T_velocity', \"minmax\"]\n",
    "\n",
    "data_resampled_dask = data_resampled_dask[training_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Velocity Max \n",
      "10T Max:  0.0045303177106794 10T Min:  -0.0054550717149153\n",
      "0.0014198635744729\n",
      "0.0001493711528956\n"
     ]
    }
   ],
   "source": [
    "print(\"Velocity Max \")\n",
    "print(\"10T Max: \", data_resampled_dask.mark_price_10T_velocity.max().compute(), \"10T Min: \",  data_resampled_dask.mark_price_10T_velocity.min().compute())\n",
    "print(data_resampled_dask.mark_price_60T_velocity.max().compute())\n",
    "print(data_resampled_dask.mark_price_1440T_velocity.max().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acceleration Min/Max\n",
      "10T Max:  0.000816395032366 10T Min:  -0.0007903738584578\n",
      "60T Max:  2.152452744598861e-05 60T Min:  -3.52456671819301e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Acceleration Min/Max\")\n",
    "print(\"10T Max: \", data_resampled_dask.mark_price_10T_acceleration_for_10T_velocity.max().compute(), \"10T Min: \",  data_resampled_dask.mark_price_10T_acceleration_for_10T_velocity.min().compute())\n",
    "print(\"60T Max: \", data_resampled_dask.mark_price_60T_acceleration_for_60T_velocity.max().compute(), \"60T Min: \",  data_resampled_dask.mark_price_60T_acceleration_for_60T_velocity.min().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STD Min/Max\n",
      "10T Max:  0.0185593805333707 10T Min:  0.0\n",
      "60T Max:  0.0272826194257299 60T Min:  4.3099397711767675e-11\n",
      "1440T Max:  0.0466893674281633 1440T Min:  2.742964702065232e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"STD Min/Max\")\n",
    "print(\"10T Max: \", data_resampled_dask.mark_price_10T_std.max().compute(), \"10T Min: \",  data_resampled_dask.mark_price_10T_std.min().compute())\n",
    "print(\"60T Max: \", data_resampled_dask.mark_price_60T_std.max().compute(), \"60T Min: \",  data_resampled_dask.mark_price_60T_std.min().compute())\n",
    "print(\"1440T Max: \", data_resampled_dask.mark_price_1440T_std.max().compute(), \"1440T Min: \",  data_resampled_dask.mark_price_1440T_std.min().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_indices = np.array(data_resampled_dask[data_resampled_dask[\"minmax\"] == 1].index.compute().tolist())\n",
    "max_indices = np.array(data_resampled_dask[data_resampled_dask[\"minmax\"] == 2].index.compute().tolist())\n",
    "neither_indices = np.array(data_resampled_dask[data_resampled_dask[\"minmax\"] == 0].index.compute().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_resampled_dask.drop(\"minmax\", axis=1).to_dask_array().compute()\n",
    "Y = data_resampled_dask[\"minmax\"].to_dask_array().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "559972"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Size: 447977\nTest Size: 111995\n"
     ]
    }
   ],
   "source": [
    "#The level 1 train/test split should NOT be shuffled! We want to test the final model on unshuffled data\n",
    "SpliteIndex = int(len(Y)*TRAIN_SIZE_CRITICAL_POINTS)\n",
    "X_train_level_1 = X[:SpliteIndex, :]\n",
    "Y_train_level_1 = Y[:SpliteIndex]\n",
    "\n",
    "X_test_level_1 = da.from_array(X[SpliteIndex:, :])\n",
    "Y_test_level_1 = da.from_array(Y[SpliteIndex:])\n",
    "\n",
    "print(\"Train Size: \" + str(len(Y_train_level_1)))\n",
    "print(\"Test Size: \" + str(len(Y_test_level_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X_train_level_1\n",
    "Y_train_level_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minIndices = np.where(Y_train_level_1 == 1)[0]\n",
    "minIndices = minIndices[np.where(minIndices >= TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE)] #only take indices above TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE (i.e. min window size). Also remove final index, if present, since cannot build the window off that. Also remove final index, if present, since cannot build the window off that\n",
    "\n",
    "maxIndices = np.where(Y_train_level_1 == 2)[0]\n",
    "maxIndices = maxIndices[np.where(maxIndices >= TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE)]   #Only take indices above TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE (i.e. min window size). Also remove final index, if present, since cannot build the window off that\n",
    "\n",
    "neitherIndices = np.where(Y_train_level_1 == 0)[0]\n",
    "neitherIndices = neitherIndices[np.where(neitherIndices >= TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE)] #Only take indices above TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE (i.e. min window size). Also remove final index, if present, since cannot build the window off that\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n3709\n3710\n440438\n"
     ]
    }
   ],
   "source": [
    "print(type(minIndices))\n",
    "print(len(minIndices))\n",
    "print(len(maxIndices))\n",
    "print(len(neitherIndices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(minIndices)\n",
    "np.random.shuffle(maxIndices)\n",
    "np.random.shuffle(neitherIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([104912, 210788,  98645, ..., 122714, 408009, 290708], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "minIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplitAClass(indexArray, splitIndex, endIndex):\n",
    "    print(splitIndex, endIndex)\n",
    "    if(np.isnan(endIndex)):\n",
    "        Train= indexArray[:splitIndex]\n",
    "        Test = indexArray[splitIndex:]\n",
    "    else:\n",
    "        Train= indexArray[:splitIndex]\n",
    "        Test = indexArray[splitIndex:endIndex]\n",
    "    print(\"Train Size: \" + str(len(Train)))\n",
    "    print(\"Test Size: \" + str(len(Test)))\n",
    "\n",
    "    return Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "array len: 3709\nsplitIndex: 2967\n2967 nan\nTrain Size: 2967\nTest Size: 742\n\narray len: 3710\nsplitIndex: 2968\n2968 nan\nTrain Size: 2968\nTest Size: 742\n\narray len: 440438\nsplitIndex: 47480\n47480 59350\nTrain Size: 47480\nTest Size: 11870\n\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "minSplitIndex = int(len(minIndices)*TRAIN_SIZE_CRITICAL_POINTS)\n",
    "maxSplitIndex = int(len(maxIndices)*TRAIN_SIZE_CRITICAL_POINTS)\n",
    "neitherEndIndex = int((minSplitIndex+maxSplitIndex)*NEITHER_SIZE_MULTIPLE)\n",
    "neitherSplitIndex = int(neitherEndIndex*TRAIN_SIZE_CRITICAL_POINTS)\n",
    "for theIndexArray, splitIndex, endIndex in zip(\n",
    "    [minIndices, maxIndices, neitherIndices],\n",
    "    [minSplitIndex, maxSplitIndex, neitherSplitIndex], \n",
    "    [np.nan, np.nan, neitherEndIndex]):\n",
    "    print(\"array len: \" + str(len(theIndexArray)))\n",
    "    print(\"splitIndex: \" + str(splitIndex))\n",
    "    trainTemp, testTemp = trainTestSplitAClass(theIndexArray, splitIndex, endIndex)\n",
    "\n",
    "    train_list.append(trainT  emp)\n",
    "    test_list.append(testTemp)\n",
    "    print(\"\")\n",
    "\n",
    "TrainIndices = np.append(train_list[0], train_list[1], axis=0)\n",
    "TrainIndices = np.append(TrainIndices, train_list[2], axis=0)\n",
    "np.random.shuffle(TrainIndices)\n",
    "\n",
    "TestIndices = np.append(test_list[0], test_list[1], axis=0)\n",
    "TestIndices = np.append(TestIndices, test_list[2], axis=0)\n",
    "np.random.shuffle(TestIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data shape: (120, 15)\n",
      "data normalized shape: (120, 15)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "data shape: (120, 15)\n",
      "data normalized shape: (120, 15)\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "<ipython-input-24-5b992f7a2d6c>:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  data_normalized = (data - data.min()) / (data.max() - data.min())\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 273915 is out of bounds for axis 0 with size 53415",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5b992f7a2d6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_Train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindowIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdata_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mX_Train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwindowIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 273915 is out of bounds for axis 0 with size 53415"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "numWindowsTrain = len(TrainIndices)\n",
    "numColumns = X.shape[1]\n",
    "X_Train = np.zeros([numWindowsTrain, TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE, numColumns])\n",
    "Y_Train = np.empty(shape=numWindowsTrain, dtype=np.int32)\n",
    "for  index, windowIndex in zip(range(0, len(TrainIndices)), TrainIndices):\n",
    "    #The final row of of the window needs to include windowIndex, so build the start and stop indices accordingly\n",
    "    windowStartIndex = windowIndex - TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE + 1\n",
    "    \n",
    "    #If the windowIndex happens to the be final index in the X array, then handle that situation\n",
    "    if(windowIndex == len(X)-1):\n",
    "        X_Train[index] = X[windowStartIndex:, :]\n",
    "    else: #This is normal situation (i.e. windowIndex is not the final index in the array)\n",
    "        windowEndIndex = windowIndex + 1\n",
    "        X_Train[index] = X[windowStartIndex:windowEndIndex, :]\n",
    "    Y_Train[index] = Y[windowIndex] #Take hte label from The final row in the window\n",
    "\n",
    "    \n",
    "    data = X_Train[windowIndex, :, :]\n",
    "    data_normalized = (data - data.min()) / (data.max() - data.min())\n",
    "    X_Train[windowIndex, :, :] = data_normalized\n",
    "    \n",
    "    print()\n",
    "    print(\"data shape: \" + str(data.shape))\n",
    "    print(\"data normalized shape: \" + str(data_normalized.shape))\n",
    "    print(data)\n",
    "    print(data_normalized)\n",
    "\n",
    "\n",
    "X_Train = da.from_array(X_Train) \n",
    "Y_Train = da.from_array(Y_Train)\n"
   ]
  },
  {
   "source": [
    "numWindowsTest = len(TestIndices)\n",
    "numColumns = X.shape[1]\n",
    "X_Test = np.zeros([numWindowsTest, TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE, numColumns])\n",
    "Y_Test = np.empty(shape=numWindowsTest, dtype=np.int32)\n",
    "for index, windowIndex in zip(range(0, len(TestIndices)), TestIndices):\n",
    "    #The final row of of the window needs to include windowIndex, so build the start and stop indices accordingly\n",
    "    windowStartIndex = windowIndex - TRAIN_TEST_SPLIT__MIN_WINDOW_SIZE + 1\n",
    "    \n",
    "    #If the windowIndex happens to the be final index in the X array, then handle that situation\n",
    "    if(windowIndex == len(X)-1):\n",
    "        X_Test[index] = X[windowStartIndex:, :]\n",
    "    else: #This is normal situation (i.e. windowIndex is not the final index in the array)\n",
    "        windowEndIndex = windowIndex + 1\n",
    "        X_Test[index] = X[windowStartIndex:windowEndIndex, :]\n",
    "    Y_Test[index] = Y[windowIndex] #Take the label from The final row in the window\n",
    "\n",
    "X_Test = da.from_array(X_Test)\n",
    "Y_Test = da.from_array(Y_Test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(53415, 120, 15)\n(53415,)\n(13354, 120, 15)\n(13354,)\n<class 'dask.array.core.Array'>\n"
     ]
    }
   ],
   "source": [
    "print(X_Train.shape)\n",
    "print(Y_Train.shape)\n",
    "print(X_Test.shape)\n",
    "print(Y_Test.shape)\n",
    "print(type(X_Train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"DataFromDeepLearningProcessing/DODGE/X_Train\"\n",
    "if(not os.path.exists(dir)):\n",
    "            os.makedirs(dir)\n",
    "da.to_npy_stack(dir, X_Train, axis=0)\n",
    "\n",
    "dir = \"DataFromDeepLearningProcessing/DODGE/Y_Train\"\n",
    "if(not os.path.exists(dir)):\n",
    "            os.makedirs(dir)\n",
    "da.to_npy_stack(dir, Y_Train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"DataFromDeepLearningProcessing/DODGE/X_Test\"\n",
    "if(not os.path.exists(dir)):\n",
    "            os.makedirs(dir)\n",
    "da.to_npy_stack(dir, X_Test, axis=0)\n",
    "\n",
    "dir = \"DataFromDeepLearningProcessing/DODGE/Y_Test\"\n",
    "if(not os.path.exists(dir)):\n",
    "            os.makedirs(dir)\n",
    "da.to_npy_stack(dir, Y_Test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"DataFromDeepLearningProcessing/DODGE/X_Test_Level_1\"\n",
    "if(not os.path.exists(dir)):\n",
    "            os.makedirs(dir)\n",
    "da.to_npy_stack(dir, X_test_level_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"DataFromDeepLearningProcessing/DODGE/Y_Test_Level_1\"\n",
    "if(not os.path.exists(dir)):\n",
    "            os.makedirs(dir)\n",
    "da.to_npy_stack(dir, Y_test_level_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}